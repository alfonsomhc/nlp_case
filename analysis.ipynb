{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb1d770-5af3-46a3-a30c-ba49e02df93a",
   "metadata": {},
   "source": [
    "# NLP Case\n",
    "In this case we will solve two NLP tasks. \n",
    "In the first one we need to find sentences that dicuss maintenance. \n",
    "In the second task we will classify documents.\n",
    "\n",
    "We start by reading the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98122ec0-8775-4139-994e-925999b8d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>pagenum</th>\n",
       "      <th>content</th>\n",
       "      <th>el_number</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00206BA4E8F5200610123622.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>K3G800-PW07-01           EC centrifugal module...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00206BA4E8F5200610123622.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>backward-curved, single-intake\\n              ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00206BA4E8F5200610123622.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>ebm-papst Mulfingen GmbH &amp; Co. KG\\n           ...</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00206BA4E8F5200610123622.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Nominal data</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00206BA4E8F5200610123622.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Type  K3G800-PW07-01\\n       Motor M3G200-QA</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file_name  pagenum  \\\n",
       "0  00206BA4E8F5200610123622.pdf        0   \n",
       "1  00206BA4E8F5200610123622.pdf        0   \n",
       "2  00206BA4E8F5200610123622.pdf        0   \n",
       "3  00206BA4E8F5200610123622.pdf        0   \n",
       "4  00206BA4E8F5200610123622.pdf        0   \n",
       "\n",
       "                                             content  el_number category  \n",
       "0  K3G800-PW07-01           EC centrifugal module...          0       36  \n",
       "1  backward-curved, single-intake\\n              ...          1       36  \n",
       "2  ebm-papst Mulfingen GmbH & Co. KG\\n           ...          2       36  \n",
       "3                                       Nominal data          3       36  \n",
       "4       Type  K3G800-PW07-01\\n       Motor M3G200-QA          4       36  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"data.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f45ec-75ee-487c-93e3-4bf24a693fe1",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "In this task we will find sentences that discuss maintenance. The suggested approach is to look for the following keywords:\n",
    "\n",
    " - “kontroll”, “vedlikehold”, “tilsyn”, \"sikkerhetskontroll”, “inspiser\", “inspeksjon”, “fjerning” (could be a suffix, eg. \"støvfjerning”), “fjerne”, “service”, “støvsuging”, “prøves\", “skiftes\", \"ettertrekking”, “ettersyn”\n",
    "\n",
    "in the same sentence as any of these phrases:\n",
    "\n",
    " - årlig, månedlig,   daglig, per år / pr år, per måned / pr måned,  halvårlig, hvert halvår, ukentlig, per uke / pr uke, hver x. til y uke (x, y = integers), hver x. til y. måned (x, y = integers), driftstime, time, en gang i året, en gang i uken, en gang i måneden.\n",
    " \n",
    "The first step is to split paragraphs into sentences. The trivial solution is to split strings by '.' characters, but there are many exceptions. It's better to use a library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f04480-f7dc-4bf4-8e49-88b7d2f5f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/norwegian.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4db70e-f611-4d4f-bf50-0b89f6994e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists\n",
    "    \"\"\"\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def make_sentences(content_string):\n",
    "    \"\"\"\n",
    "    Take a string as an input and return a list of sentences\n",
    "    \"\"\"\n",
    "    lines = content_string.splitlines()\n",
    "    sentences = [tokenizer.tokenize(l) for l in lines]\n",
    "    return flatten(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b835f-ae16-408c-8ff3-d2a89abd1d69",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e846344-c025-4bd2-9573-5bc125bd362d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeg spiser i dag.', 'Jeg spiser hver 5. minutt.', 'Ny linje:', 'Hei hei']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_string = \"Jeg spiser i dag. Jeg spiser hver 5. minutt. Ny linje:\\nHei hei\"\n",
    "make_sentences(content_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2285850-54a4-4c33-a67e-6353293f9a77",
   "metadata": {},
   "source": [
    "Apply this function to our dataset (content column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9747c18b-7fb3-472d-8006-442b0f9ab268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    K3G800-PW07-01           EC centrifugal module...\n",
       "1                       backward-curved, single-intake\n",
       "2                       ebm-papst Mulfingen GmbH & Co.\n",
       "3                                         Nominal data\n",
       "4                                 Type  K3G800-PW07-01\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = (\n",
    "    df.content\n",
    "    .apply(make_sentences)\n",
    "    .apply(lambda x: x[0])\n",
    ")\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616204d-5551-46f6-9fee-14fa76fac03e",
   "metadata": {},
   "source": [
    "Finally search for sentences that follow the specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b744ac-5296-4214-ac8d-f650c731cc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15560    Sikkerhetsrapport årlig kontroll brannalarmanl...\n",
       "15570    Sikkerhetsrapport årlig kontroll brannalarmanl...\n",
       "15572      Kontrollseddel årlig kontroll brannalarmanlegg:\n",
       "15579      Kontrollseddel årlig kontroll brannalarmanlegg:\n",
       "15581     Kontrollrapport årlig kontroll brannalarmanlegg:\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = [\"årlig\", \"månedlig\", \"daglig\", \"per år\", \"pr år\", \"per måned\", \"pr måned\", \n",
    "              \"halvårlig\", \"hvert halvår\", \"ukentlig\", \"per uke\", \"pr uke\", \"driftstime\", \n",
    "              \"time\", \"en gang i året\", \"en gang i uken\", \"en gang i måneden\", \n",
    "              \"hver \\d. til \\d. uke\", \"hver \\d. til \\d. måned\"]\n",
    "keywords = [\"kontroll\", \"vedlikehold\", \"tilsyn\", \"sikkerhetskontroll\", \n",
    "             \"inspiser\", \"inspeksjon\", \"fjerning\", \"fjerne\", \"service\", \n",
    "             \"støvsuging\", \"prøves\", \"skiftes\", \"ettertrekking\", \"ettersyn\"]\n",
    "filter1 = sentences.str.contains('|'.join(candidates))\n",
    "filter2 = sentences.str.contains('|'.join(keywords))\n",
    "\n",
    "sentences_maintenance = sentences[filter1 & filter2]\n",
    "sentences_maintenance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870437d0-7291-4072-a479-bec0e8390b08",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "In this task we will classify documents. \n",
    "The first step is to merge paragraphs into documents.\n",
    "\n",
    "Some documents belong to multiple categories for some reason. This could be an error, or maybe some documents have sections that belong to one class or another. Since we don't know why, we can just use the first category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f896782b-a735-41a6-91c0-0180d49b34bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document '15092006 Samsvarserklæring på Høyttalerjobb i bg14.pdf' has 2 categories\n",
      "Document '171016.sk bla A51 anbefalt oppgradering tekniske anlegg i forbindelse med nye leietakere.pdf' has 2 categories\n",
      "Document '1_Ferdigattest.pdf' has 2 categories\n",
      "Document '20_02714-15GNR 137 BNR 3 - Brynsengfaret 4 - Sikring mot tilbakestrømming - Tilbakeslagsventiler - Bilde av tilkobling til fettu.pdf' has 2 categories\n",
      "Document '220506,samsvarserklæring på høyttalerjobb bg14.pdf' has 2 categories\n",
      "Document '254.02 Fugemasse SB Flex (245.02).pdf' has 2 categories\n",
      "Document 'AX flow reduksjonsventil 000300.pdf' has 2 categories\n",
      "Document 'Adresseliste Akersgata 51.pdf' has 2 categories\n",
      "Document 'EV220B 6-12 engelsk.pdf' has 2 categories\n",
      "Document 'FDV XMS 0-4 Oslomiks med Oldroyd og 10mm VT-filt_Blomstertak AS.pdf' has 2 categories\n",
      "Document 'FDV-dokumentasjon.pdf' has 2 categories\n",
      "Document 'FDV.pdf' has 2 categories\n",
      "Document 'Ferdigattest - gnr-bnr 24-547 - solenergianlegg.PDF' has 2 categories\n",
      "Document 'HMS RAD porefyller 1.pdf' has 2 categories\n",
      "Document 'RAD porefyller 1 manual.pdf' has 2 categories\n",
      "Document 'Samværserklæring A51.pdf' has 2 categories\n",
      "Document 'Termografi rapport Statsbygg BG 6  pdf.pdf' has 2 categories\n",
      "Document 'Thermo control - 42N.pdf' has 2 categories\n",
      "Document 'Thermo control - 42gw.pdf' has 2 categories\n",
      "Document 'Thermo control - Himod Service Manual Svensk.pdf' has 2 categories\n",
      "Document 'Thermo control - KLR-E_7000.pdf' has 2 categories\n",
      "Document '03.00 Funksjonsbeskrivelse.pdf' has 2 categories\n",
      "Document '04.00 Automatikk_tavleskjema.pdf' has 2 categories\n",
      "Document '04.04 P169010120-360.04 - 350.01 Ventilasjon og kjølesentral skjema asbuilt.pdf' has 2 categories\n",
      "Document '05.00 Feilsøking og nødprosedyrer.pdf' has 2 categories\n",
      "Document '06.00 Tilsyn og vedlikehold.pdf' has 2 categories\n",
      "Document '07.00 Brukermanualer.pdf' has 2 categories\n",
      "Document '07.06 FEC Betjeningsmanual.pdf' has 2 categories\n",
      "Document '08.00 Brukermanualer sd-anlegg.pdf' has 2 categories\n",
      "Document '08.04 MSEA 5.0 Brukermanual.pdf' has 2 categories\n",
      "Document '09 DP2500-R8-D Differanstrykkføler.pdf' has 2 categories\n",
      "Document '09 FEC26 Controller FDV_no.pdf' has 2 categories\n",
      "Document '09 P233A_Difftrykk_luft.pdf' has 2 categories\n",
      "Document '09 P233A_mont.anvisning.pdf' has 2 categories\n",
      "Document '09 TS-6300 Temperaturfølere.pdf' has 2 categories\n",
      "Document '09 TS9100 Tempfølere kanal_væske.pdf' has 2 categories\n",
      "Document '09.00 Teknisk dokumentasjon.pdf' has 2 categories\n",
      "Document '10 Grundfos TP 32-180-2.pdf' has 2 categories\n",
      "Document '10.00 Igangkjøringsprotokoller etc.pdf' has 2 categories\n",
      "Document '3 STAD innreguleringsventil.pdf' has 2 categories\n",
      "Document '4 STAF innreguleringsventil.pdf' has 2 categories\n",
      "Document '6 Armaflex isolasjon.pdf' has 2 categories\n",
      "Document 'AM.pdf' has 2 categories\n",
      "Document 'Arealrapport Langkaia 1 - REV 7.pdf' has 2 categories\n",
      "Document 'Innreguleringsprotokoll - Rør.pdf' has 2 categories\n",
      "Document 'Innreguleringsprotokollen 9. etg..pdf' has 2 categories\n",
      "Document 'RammetillatelseLK1.PDF' has 2 categories\n",
      "Document 'TS9100.pdf' has 2 categories\n",
      "Document 'EUNOBE-00050471 - 1 av 1 - 20211019 1212.pdf' has 2 categories\n",
      "Document 'Fredningsdokument.pdf' has 2 categories\n",
      "Document 'T5 El.ballast Helvar.pdf' has 2 categories\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "documents = []\n",
    "categories = []\n",
    "\n",
    "for f in df.file_name.unique():\n",
    "    paragraph_list = df.loc[df.file_name == f, :].sort_values(by='el_number')\n",
    "    doc = ' '.join(paragraph_list.content)\n",
    "    cat = paragraph_list.category.iloc[0]\n",
    "    \n",
    "    if (ncat:=paragraph_list.category.nunique()) != 1:\n",
    "        print(f\"Document '{f}' has {ncat} categories\")\n",
    "    \n",
    "    documents.append(doc)\n",
    "    categories.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47b51b0-40b2-4b2c-b47b-a2804bf88f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = pd.Series(categories)\n",
    "categories.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23359d71-90db-45cf-900e-4eabc321b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '210',\n",
       " '22',\n",
       " '23',\n",
       " '233',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '49',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '542',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '55',\n",
       " '552',\n",
       " '556',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '61',\n",
       " '62',\n",
       " '620',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '71',\n",
       " '74',\n",
       " '76',\n",
       " '77']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(categories.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c6b904c-5c18-47c3-aa8a-318091a72c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    1364\n",
       "43     924\n",
       "40     888\n",
       "36     752\n",
       "24     607\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.value_counts(dropna = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eb4e66-f838-411e-abf5-4e3724b26941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21     16\n",
       "66     13\n",
       "65     11\n",
       "51     10\n",
       "77     10\n",
       "10      9\n",
       "547     8\n",
       "552     7\n",
       "548     5\n",
       "16      5\n",
       "34      3\n",
       "49      2\n",
       "210     2\n",
       "233     2\n",
       "74      2\n",
       "42      2\n",
       "18      2\n",
       "71      1\n",
       "620     1\n",
       "76      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.value_counts(dropna = False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4750e-4276-4f05-b061-70edd9bf018e",
   "metadata": {},
   "source": [
    "It would be a good idea to understand those categories. Can some categories be merged? Some have many instances, while others have very few (e.g. '76' has only 1 instance).\n",
    "\n",
    "I will map all classes with less than 10 instances to an \"other\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e94756-1b4b-4362-bfdb-d88cf4ad0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = categories.value_counts()\n",
    "other_categories = category_counts[category_counts < 10].index.values\n",
    "other_index = categories.isin(other_categories)\n",
    "categories[other_index] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1edd4-57b1-445d-961d-37b6dafdb63b",
   "metadata": {},
   "source": [
    "Split into training and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2ce3ea-734b-42ed-bc90-cf20b7d35389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documents, categories, test_size=0.33, random_state=42, stratify=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec65df7-cb13-46c3-b107-78da2979f808",
   "metadata": {},
   "source": [
    "We will use a classical classification model: TFIDF features and an SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b7dfb1-8449-4b1a-a616-d0a7d47b97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_vect_train = vectorizer.fit_transform(X_train)\n",
    "X_vect_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac98f70c-701e-45f1-b7bb-d51126c7b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, tol=None).fit(X_vect_train, y_train)\n",
    "y_train_hat = model.predict(X_vect_train)\n",
    "y_test_hat = model.predict(X_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f96f7e-d044-4cdb-bbad-62eab3e3c2aa",
   "metadata": {},
   "source": [
    "Measure performance on the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a80949a7-e827-426a-8a55-30e444847494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        35\n",
      "          11       0.99      0.99      0.99       914\n",
      "          12       0.96      0.79      0.87        29\n",
      "          13       0.82      1.00      0.90        45\n",
      "          14       0.94      0.74      0.83        23\n",
      "          15       0.70      0.96      0.81        70\n",
      "          19       0.94      0.88      0.91       138\n",
      "          20       0.96      0.77      0.86        62\n",
      "          21       1.00      0.91      0.95        11\n",
      "          22       1.00      0.88      0.94        17\n",
      "          23       0.97      0.90      0.94       189\n",
      "          24       0.94      0.97      0.96       407\n",
      "          25       0.95      0.95      0.95       225\n",
      "          26       1.00      0.94      0.97        62\n",
      "          27       1.00      0.94      0.97       173\n",
      "          28       0.91      0.93      0.92        54\n",
      "          29       0.93      0.98      0.95       157\n",
      "          30       1.00      0.07      0.13        14\n",
      "          31       0.97      0.90      0.93       159\n",
      "          32       0.82      0.86      0.84       130\n",
      "          33       0.98      0.98      0.98       124\n",
      "          35       0.96      0.76      0.85        97\n",
      "          36       0.83      0.97      0.89       504\n",
      "          37       0.83      0.62      0.71        24\n",
      "          40       0.95      0.98      0.96       595\n",
      "          41       0.87      0.61      0.72        96\n",
      "          43       0.90      0.96      0.93       619\n",
      "          44       0.95      0.96      0.95       271\n",
      "          45       1.00      0.80      0.89        15\n",
      "          46       0.97      0.55      0.70        56\n",
      "          47       0.97      0.91      0.94        70\n",
      "          51       1.00      0.71      0.83         7\n",
      "          52       0.91      0.86      0.89        59\n",
      "          53       1.00      1.00      1.00        23\n",
      "          54       0.93      0.93      0.93       191\n",
      "         542       1.00      0.74      0.85        23\n",
      "         546       0.98      0.98      0.98        51\n",
      "          55       1.00      0.85      0.92        26\n",
      "         556       1.00      0.92      0.96        24\n",
      "          56       0.92      0.85      0.88       203\n",
      "          57       0.88      0.74      0.81        86\n",
      "          58       0.64      0.97      0.77        30\n",
      "          61       1.00      0.92      0.96        78\n",
      "          62       0.96      0.99      0.97       144\n",
      "          65       1.00      1.00      1.00         7\n",
      "          66       1.00      0.89      0.94         9\n",
      "          67       0.96      0.92      0.94        60\n",
      "          77       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.93      6413\n",
      "   macro avg       0.94      0.87      0.89      6413\n",
      "weighted avg       0.93      0.93      0.93      6413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, y_train_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a953ed9-b3d4-48cc-ad05-46731ae85a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.21        17\n",
      "          11       0.96      0.99      0.97       450\n",
      "          12       0.80      0.53      0.64        15\n",
      "          13       0.63      0.83      0.72        23\n",
      "          14       0.75      0.25      0.38        12\n",
      "          15       0.62      0.88      0.73        34\n",
      "          19       0.74      0.63      0.68        68\n",
      "          20       0.75      0.29      0.42        31\n",
      "          21       0.00      0.00      0.00         5\n",
      "          22       0.83      0.56      0.67         9\n",
      "          23       0.70      0.53      0.61        94\n",
      "          24       0.70      0.80      0.74       200\n",
      "          25       0.77      0.77      0.77       111\n",
      "          26       1.00      0.50      0.67        30\n",
      "          27       0.85      0.66      0.74        85\n",
      "          28       0.86      0.73      0.79        26\n",
      "          29       0.73      0.94      0.82        77\n",
      "          30       0.00      0.00      0.00         7\n",
      "          31       0.82      0.59      0.69        78\n",
      "          32       0.60      0.55      0.57        64\n",
      "          33       0.85      0.72      0.78        61\n",
      "          35       1.00      0.33      0.50        48\n",
      "          36       0.64      0.91      0.75       248\n",
      "          37       1.00      0.17      0.29        12\n",
      "          40       0.89      0.96      0.92       293\n",
      "          41       0.72      0.49      0.58        47\n",
      "          43       0.72      0.90      0.80       305\n",
      "          44       0.75      0.87      0.81       134\n",
      "          45       1.00      0.57      0.73         7\n",
      "          46       0.60      0.22      0.32        27\n",
      "          47       1.00      0.76      0.87        34\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.71      0.69      0.70        29\n",
      "          53       0.78      0.64      0.70        11\n",
      "          54       0.72      0.74      0.73        94\n",
      "         542       1.00      0.17      0.29        12\n",
      "         546       0.86      0.72      0.78        25\n",
      "          55       0.67      0.31      0.42        13\n",
      "         556       0.60      0.25      0.35        12\n",
      "          56       0.59      0.51      0.55       100\n",
      "          57       0.76      0.65      0.70        43\n",
      "          58       0.68      1.00      0.81        15\n",
      "          61       0.82      0.69      0.75        39\n",
      "          62       0.73      0.92      0.81        71\n",
      "          65       1.00      0.25      0.40         4\n",
      "          66       1.00      0.50      0.67         4\n",
      "          67       0.75      0.50      0.60        30\n",
      "          77       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.77      3160\n",
      "   macro avg       0.74      0.56      0.60      3160\n",
      "weighted avg       0.78      0.77      0.76      3160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acanterla/.asdf/installs/python/3.9.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/acanterla/.asdf/installs/python/3.9.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/acanterla/.asdf/installs/python/3.9.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569378d-6454-4349-ba95-8b58a6ead202",
   "metadata": {},
   "source": [
    "Performance varies from class to class. When the number of instances in the test set is low, the performance estimate is probably not statistically significant. We could have considered other methods such as cross validation to have a better performance estimate.\n",
    "\n",
    "The process of generating TFIDF vectors has a lot of parameters that can be optimized. I'd have liked to look into char level ngrams. This works well in language like Norwegian where there are compound words.\n",
    "\n",
    "The dataset is quite unbalanced, as we have seen. Some classes have thousands of instances, while some others have less than twenty. There are methods to deal with this issue. For example, the model that we used has a class_frequency parameter that can be used for this purpose. We leave this for future work as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
